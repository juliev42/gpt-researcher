<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd"> 
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1-3.dtd?><?SourceDTD.Version 1.3?><?ConverterInfo.XSLTName jats2jats3.xsl?><?ConverterInfo.Version 1?><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Diagnostics (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Diagnostics (Basel)</journal-id><journal-id journal-id-type="publisher-id">diagnostics</journal-id><journal-title-group><journal-title>Diagnostics</journal-title></journal-title-group><issn pub-type="epub">2075-4418</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">10378334</article-id><article-id pub-id-type="doi">10.3390/diagnostics13142315</article-id><article-id pub-id-type="publisher-id">diagnostics-13-02315</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Use Test of Automated Machine Learning in Cancer Diagnostics</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Musigmann</surname><given-names>Manfred</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><xref rid="c1-diagnostics-13-02315" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Nacul</surname><given-names>Nabila Gala</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role></contrib><contrib contrib-type="author"><name><surname>Kasap</surname><given-names>Dilek N.</given-names></name></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-6562-280X</contrib-id><name><surname>Heindel</surname><given-names>Walter</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-9335-9546</contrib-id><name><surname>Mannil</surname><given-names>Manoj</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Chen</surname><given-names>Dechang</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-diagnostics-13-02315">University Clinic for Radiology, University Hospital Muenster, WWU Muenster, Albert-Schweitzer-Campus 1, DE-48149 Muenster, Germany</aff><author-notes><corresp id="c1-diagnostics-13-02315"><label>*</label>Correspondence: <email>musigma@uni-muenster.de</email></corresp></author-notes><pub-date pub-type="epub"><day>08</day><month>7</month><year>2023</year></pub-date><pub-date pub-type="collection"><month>7</month><year>2023</year></pub-date><volume>13</volume><issue>14</issue><elocation-id>2315</elocation-id><history><date date-type="received"><day>31</day><month>5</month><year>2023</year></date><date date-type="rev-recd"><day>30</day><month>6</month><year>2023</year></date><date date-type="accepted"><day>03</day><month>7</month><year>2023</year></date></history><permissions><copyright-statement>&#x000a9; 2023 by the authors.</copyright-statement><copyright-year>2023</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Our aim is to investigate the added value of automated machine learning (AutoML) for potential future applications in cancer diagnostics. Using two important diagnostic questions, the non-invasive determination of IDH mutation status and ATRX status, we analyze whether it is possible to use AutoML to develop models that are comparable in performance to conventional machine learning models (ML) developed by experts. For this purpose, we develop AutoML models using different feature preselection methods and compare the results with previously developed conventional ML models. The cohort used for our study comprises T2-weighted MRI images of 124 patients with histologically confirmed gliomas. Using AutoML, we were able to develop sophisticated models in a very short time with only a few lines of computer code. In predicting IDH mutation status, we obtained a mean AUC of 0.7400 and a mean AUPRC of 0.8582. ATRX mutation status was predicted with very similar discriminatory power, with a mean AUC of 0.7810 and a mean AUPRC of 0.8511. In both cases, AutoML was even able to achieve a discriminatory power slightly above that of the respective conventionally developed models in a very short computing time, thus making such methods accessible to non-experts in the near future.</p></abstract><kwd-group><kwd>machine learning</kwd><kwd>AutoML</kwd><kwd>radiomics</kwd><kwd>MRI</kwd></kwd-group><funding-group><funding-statement>This research received no external funding.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-diagnostics-13-02315"><title>1. Introduction</title><p>Artificial intelligence (AI) techniques and their applications are an important current topic in medical research. In recent years, medical research in the field of AI has increased enormously [<xref rid="B1-diagnostics-13-02315" ref-type="bibr">1</xref>]. Many corresponding applications have already become an indispensable part of everyday medical life. Machine learning (ML) methods are an important subcategory of artificial intelligence. These techniques attempt to mimic human learning using suitable algorithms. Basically, ML refers to the use and development of computer systems that are able to learn autonomously using suitable algorithms in order to analyze patterns in data and draw appropriate conclusions. In our current study, we focus on so-called &#x0201c;supervised machine learning&#x0201d;, or more precisely on &#x0201c;automated supervised machine learning&#x0201d;. &#x0201c;Supervised&#x0201d; means that these algorithms are first trained with known data and associated outcomes to be predicted in order to recognize relevant facts for the decisions to be made later. Subsequently, supervised machine learning algorithms are able to apply the previously learned logic accordingly to unknown data and thus make decisions independently. &#x0201c;Deep learning&#x0201d;, in turn, is an important subcategory of supervised machine learning. These algorithms use artificial neural networks, which often contain many layers. They are usually trained with a large amount of data.</p><p>Non-invasive methods are of great importance for the gentle treatment of patients. This applies, for example, to tumor diagnostics and, in particular, to tumors of the central nervous system (CNS). Artificial intelligence techniques and especially machine learning algorithms offer an enormous potential for this important challenge. Quantitative factors extracted from conventional MRI and CT images (so-called radiomic features) can be used in conjunction with machine learning algorithms to answer important diagnostic questions in the diagnosis of brain tumors in a fully automated, objective, highly precise, and, above all, non-invasive way. For example, Ari et al. showed that radiomic-based machine learning can be used to non-invasively predict pseudoprogression in high-grade gliomas [<xref rid="B2-diagnostics-13-02315" ref-type="bibr">2</xref>]. Kr&#x000e4;hling et al. developed an MRI-based radiomics model to predict mitotic cycles in intracranial meningiomas before surgery [<xref rid="B3-diagnostics-13-02315" ref-type="bibr">3</xref>]. In Musigmann et al., it was shown that ML algorithms can predict possible total and subtotal resections of skull meningiomas using pre-treatment T1 post-contrast MR images [<xref rid="B4-diagnostics-13-02315" ref-type="bibr">4</xref>].</p><p>These and many other studies demonstrate the enormous potential of such techniques. However, as promising as these techniques using ML algorithms may be, it is often difficult for non-experts to apply them correctly. The preparation of data, the selection of features to be included in a model, the determination of a model algorithm suitable for the respective problem, and the optimization of the parameters included in the model often require a considerable amount of expert knowledge. All these and many other steps are difficult to apply correctly and, above all, are often very time-consuming. Moreover, the number of experts capable of developing stable, high-performance models is very limited. This is where the idea of the next &#x0201c;generation&#x0201d; of machine learning comes in, the so-called &#x0201c;automated machine learning&#x0201d; (AutoML). AutoML is a novel tool in the broad field of AI. A major goal of AutoML is to simplify and automate numerous steps in model development to make ML algorithms also accessible to non-experts. But the high level of automation also offers numerous potential advantages for experts. For example, AutoML can be used to get a quick overview of which model algorithms might be promising for further detailed analysis.</p><p>Despite the rapidly increasing demand for this comparatively young technology, there have been relatively few publications on automated machine learning in the medical sector. Ikemura et al. used AutoML to predict patients&#x02019; chances of surviving a SARS-CoV-2 infection [<xref rid="B5-diagnostics-13-02315" ref-type="bibr">5</xref>]. Karaglani et al. produced predictive biosignatures that provide opportunities for minimally invasive blood-based diagnostic tests for Alzheimer&#x02019;s disease [<xref rid="B6-diagnostics-13-02315" ref-type="bibr">6</xref>]. Ou et al. used AutoML for intracranial aneurysm treatment outcome prediction and compared the results with those obtained with multivariate logistic regression and a manually developed ML model [<xref rid="B7-diagnostics-13-02315" ref-type="bibr">7</xref>]. Touma et al. developed a completely code-free AutoML model with very high accuracy for classifying cataract surgery phases from videos [<xref rid="B8-diagnostics-13-02315" ref-type="bibr">8</xref>]. Initial studies like these show the great potential that current AutoML algorithms already offer. However, there have been few efforts to apply these techniques in the health sector yet [<xref rid="B9-diagnostics-13-02315" ref-type="bibr">9</xref>,<xref rid="B10-diagnostics-13-02315" ref-type="bibr">10</xref>]. A good overview of the basic methodology, the state of the art, and the possibilities of automated machine learning in healthcare is given by Waring et al. [<xref rid="B9-diagnostics-13-02315" ref-type="bibr">9</xref>].</p><p>In this study, we address the question of how powerful and timesaving AutoML algorithms currently are. Is it already feasible to use automated machine learning algorithms to develop reliable models for important questions in medical diagnostics? Are we able to develop models in a much shorter time that nevertheless show comparable performance to models developed in a conventional way? How much expert knowledge is still necessary, which steps can be simplified, and how stable are the models obtained? We analyze these issues using an important task in cancer diagnostics that has already been studied in numerous medical publications using conventional (non-automated) machine learning algorithms as well as deep learning: the automatic classification of brain tumors according to the WHO classification of CNS tumors [<xref rid="B11-diagnostics-13-02315" ref-type="bibr">11</xref>,<xref rid="B12-diagnostics-13-02315" ref-type="bibr">12</xref>,<xref rid="B13-diagnostics-13-02315" ref-type="bibr">13</xref>,<xref rid="B14-diagnostics-13-02315" ref-type="bibr">14</xref>,<xref rid="B15-diagnostics-13-02315" ref-type="bibr">15</xref>,<xref rid="B16-diagnostics-13-02315" ref-type="bibr">16</xref>].</p><p>For our analyses performed in this study, we rely on the latest version of the WHO classification for CNS tumors, which was recently published [<xref rid="B17-diagnostics-13-02315" ref-type="bibr">17</xref>,<xref rid="B18-diagnostics-13-02315" ref-type="bibr">18</xref>,<xref rid="B19-diagnostics-13-02315" ref-type="bibr">19</xref>]. In this 2021 version of the WHO classification, tumors are even more consistently subdivided into molecularly and biologically defined entities. Diffuse gliomas of the adult type are primarily divided into the three main tumor types: &#x0201c;astrocytoma, IDH mutated&#x0201d;, &#x0201c;oligodendroglioma, IDH mutated and 1p/19q co-deleted&#x0201d; and &#x0201c;glioblastoma, IDH wild type&#x0201d;. A seven-step diagnostic algorithm can now be used to accurately classify diffuse gliomas of the adult type [<xref rid="B20-diagnostics-13-02315" ref-type="bibr">20</xref>]. The first two of these seven stages are particularly important. In these first two stages, mutations in isocitrate dehydrogenase (IDH) genes IDH1 and IDH2 and mutation/loss of alpha-thalassemia/mental retardation syndrome X-linked (ATRX) expression are considered. A distinction is made between &#x0201c;IDH mutant&#x0201d; and &#x0201c;IDH wild type&#x0201d; (first stage) and between &#x0201c;Nuclear ATRX retained&#x0201d; and &#x0201c;Nuclear ATRX lost&#x0201d; in the second stage. Using this particularly important task, we would like to conduct a realistic use test of AutoML. To enable this, we have previously predicted both IDH and ATRX status with conventional machine learning algorithms. However, it should be explicitly noted that the purpose of our analyses here is not primarily to be able to predict IDH and ATRX status with AutoML particularly accurately. Rather, we want to analyze how easy/difficult or time-consuming it is to achieve comparable results with AutoML as with conventional ML algorithms in order to determine the possible added value of AutoML. Our main aim is to investigate whether non-experts can already use AutoML to develop similarly good models for applications in medical diagnostics as technically skilled developers of conventional machine learning models.</p></sec><sec id="sec2-diagnostics-13-02315"><title>2. Materials and Methods</title><p>This single-center study was performed in compliance with the Declaration of Helsinki and approved by the local ethics committee (&#x000c4;rztekammer Westfalen Lippe and University of M&#x000fc;nster, 2021-596-f-S). Due to the retrospective nature of the study, written informed consent was waived by the &#x000c4;rztekammer Westfalen Lippe and University of M&#x000fc;nster.</p><p>We retrospectively searched our database for patients diagnosed with glioma between June 2008 and April 2021. Initially, 136 patients were screened using the T2 sequence, of whom 12 had to be excluded from our study because the IDH mutation status and/or the ATRX status were unknown. IDH1 and 2 mutation status was obtained according to Yan et al. [<xref rid="B21-diagnostics-13-02315" ref-type="bibr">21</xref>]. The ATRX gene provides instructions for making a protein that plays an essential role in normal development. Its mutation status was analyzed using immunohistochemistry. Our final study cohort of 124 patients was divided into 55 females/69 males, or 43/81 patients with IDH mutation status wild type/mutated, or 75/49 patients with ATRX status retained/lost. The demographic characteristics of the patients in our study cohort are summarized in <xref rid="diagnostics-13-02315-t001" ref-type="table">Table 1</xref> in relation to IDH mutation status and correspondingly in <xref rid="diagnostics-13-02315-t002" ref-type="table">Table 2</xref> in relation to ATRX status.</p><p>We performed the segmentation of the contrast-enhancing parts of the tumors semi-automatically using the open-source software platform 3D Slicer (version 4.10, <uri xlink:href="www.slicer.org">www.slicer.org</uri>, accessed on 1 June 2023). <xref rid="diagnostics-13-02315-f001" ref-type="fig">Figure 1</xref> shows an example of semi-automatic segmentation with 3D Slicer for an oligodendroglioma (IDH-mutant, ATRX retained, 1p/19q-codeleted). For each of the 124 patients, 107 radiomic features were extracted from the corresponding MRI images by hand-delineated regions of interest (ROI). Radiomic features are reproducibly quantified information derived from image morphological criteria calculated with suitable mathematical algorithms. A good overview of radiomic features and their significance is given by van Griethuysen et al. [<xref rid="B22-diagnostics-13-02315" ref-type="bibr">22</xref>]. By means of radiomic features, for example, the determination of tumor patterns and characteristics can be facilitated. In addition to classical parameters such as tumor diameter or tumor volume, other properties such as shape or heterogeneity within the tumor can also be determined as quantitative features, enabling a non-invasive characterization of tumor-suspicious lesions. To extract the 107 radiomic features from the MRI images, we used the open-source platform PyRadiomics. The PyRadiomics package is available as an implementable plugin for the 3D Slicer platform. The total of 107 radiomic features we used for our analyses is composed of 18 first-order statistics features, 14 shape-based features, 24 gray-level co-occurrence matrix features, 16 gray-level run-length matrix features, 16 gray-level size zone matrix features, 5 neighboring gray-tone difference matrix features, and 14 gray-level dependency matrix features. The exact calculation method for each feature can be found on the PyRadiomics homepage (pyradiomics.readthedocs.io). In addition to the 107 radiomic features, our database contained the gender and age of the patients at the time of diagnosis. Beyond that, no other features are used. The features used therefore consist largely of radiomic features. All features were z-score transformed and subjected to a 95% correlation filter to account for redundancy between the features.</p><p>To evaluate the performance achieved with AutoML, we have previously developed conventional ML models for both IDH mutation status prediction and ATRX status prediction. The best machine learning models that we calculated beforehand were obtained using Lasso (least absolute shrinkage and selection operator) regression. We also tested many other conventional machine learning algorithms, such as random forest, bagged trees, naive Bayes, linear discriminant analysis (LDA), support vector machines (SVMs), XGBoost, and a neural net. However, the best results overall were obtained with Lasso regression (for both IDH mutation status prediction and ATRX status). The results obtained with XGBoost were also promising and very close to those obtained with Lasso regression. Currently, there are several software packages such as AutoXGboost and Remix AutoML for automated machine learning. The individual applications usually differ in their degree of automation, their optimization speed, the options for feature preparation and preselection, and the machine learning algorithms contained. Our aim was to test an open-source application that is accessible to everyone. We decided to use &#x0201c;H2O AutoML&#x0201d;, the automated machine learning package, which is available on CRAN. However, it should be explicitly noted here that in addition to the open-source version &#x0201c;H2O AutoML&#x0201d;, a commercial version &#x0201c;H2O Driverless AI&#x0201d; is also available, which contains additional tools. We particularly liked the special feature of the open-source application H2O AutoML which allows the number of models to be trained and the time required for this to be explicitly specified. However, in our view, one disadvantage of H2O AutoML is that the possibilities for feature preparation and suitable preselection of features are still limited, at least for the moment. Therefore, for this use test, we also tested different methods for feature preselection in combination with AutoML. Specifically, we tested AutoML without feature preselection, as well as in combination with Lasso regression and recursive feature elimination (RFE) for feature preselection (see, for example, Darst et al. [<xref rid="B23-diagnostics-13-02315" ref-type="bibr">23</xref>]).</p><p>Following the documentation, H2O AutoML simultaneously optimizes models belonging to different algorithm classes: a fixed grid of GLMs (generalized linear models), five pre-specified H2O GBMs (gradient boosting machines) and a random grid of H2O GBMs, three pre-specified XGBoost GBMs and a random grid of XGBoost GBMs, a default random forest (DRF), an extremely randomized forest (XRT), a near-default deep neural net (DNN), and a random grid of deep neural nets [<xref rid="B24-diagnostics-13-02315" ref-type="bibr">24</xref>]. In addition, H2O AutoML uses combinations of these base algorithms, so-called &#x0201c;Stacked Ensemble Models (SEMs)&#x0201d;. In our opinion, these ensemble learners (SEMs) have the advantage that they can further strengthen the model quality, but at the same time, they have the disadvantage that the comprehensibility of the models can suffer in the sense of a black-box character. For our use test, we used all AutoML algorithms except XGBoost GBMs. The functionality of AutoML is already very extensive and is apparently still being further developed. We therefore explicitly refer to the respective current documentations of AutoML, as the numerous possibilities can only be hinted at here [<xref rid="B24-diagnostics-13-02315" ref-type="bibr">24</xref>].</p><sec><title>Statistical Analysis</title><p>We performed our statistical analysis using R software (version 4.1.2). The main packages used were &#x0201c;caret&#x0201d; and &#x0201c;xgboost&#x0201d; for the conventional machine learning models computed for comparison purposes and &#x0201c;H2O&#x0201d; for automated machine learning. We randomly divided the final cohort of 124 patients into training data and independent test data using a stratified ratio of 80:20 with a balanced distribution of IDH mutation status and ATRX status, respectively (compare <xref rid="diagnostics-13-02315-t001" ref-type="table">Table 1</xref> and <xref rid="diagnostics-13-02315-t002" ref-type="table">Table 2</xref>). All features used for model development were first subjected to a 95% correlation filter to account for redundancies between features. Subsequently, three methods for possible feature preselection in combination with AutoML were tested and compared. In the first method, all remaining features were used for further modeling with AutoML. The second method used Lasso regression in combination with the &#x0201c;VarImp&#x0201d; function. The VarImp function determines the performance gain/loss by adding/removing a specific feature in relation to a given model and determines which features are particularly important in this way. In this second method, we developed models with an increasing number of the most important features and then determined the optimal number of features to include based on the performance achieved (compare Musigmann et al. [<xref rid="B25-diagnostics-13-02315" ref-type="bibr">25</xref>]). Finally, in the third method, we determined the most promising features using RFE. RFE was performed using the random forest algorithm. The algorithm was simultaneously used to determine the optimal number of features to be used. The three methods for possible feature preselection differ in the sense that in the first method, neither the most promising features nor the number of features to be included in the model are determined based on the training data before the modeling process is initiated with the conventional ML algorithms or with AutoML. In the second method (using Lasso regression), the most promising features are first identified, but the exact number of features to be included is then determined in a separate second step. In this second step, the number of features included is further increased until an optimal number is reached in terms of model performance. In the third method (RFE), on the other hand, the most promising features and their optimal number are determined in a single step.</p><p>The preselection of the features, the model development (training), and the determination of the hyperparameters included in the models were performed using the training data. The hyperparameters were determined using a grid search with 10-fold cross-validation in the case of conventional machine learning models and completely automatically in the case of using AutoML. The discriminatory power of the models was subsequently determined using the unknown/independent test data. Each model was 100 times completely developed and subsequently tested, with a new split of training data and independent test data in each of these cycles. This step was conducted to exclude random effects related to the data partitioning used. All performance values were calculated as the average of these 100 cycles, with the individual results for each of these cycles determined using the associated independent test data. This procedure is described in detail in Musigmann et al. [<xref rid="B25-diagnostics-13-02315" ref-type="bibr">25</xref>]. For a better understanding, we have also summarized the entire process in a flowchart in <xref rid="diagnostics-13-02315-f002" ref-type="fig">Figure 2</xref>.</p><p>The performance achieved with the different modeling approaches was compared using the area under the curve (AUC) of the receiver operating characteristic (ROC). It should be noted that in this study we do not use other commonly used metrics such as accuracy, sensitivity, and specificity. We do not use these metrics in this study because they are highly dependent on the threshold used to distinguish the classes to be predicted. This is especially true in cases with a strong imbalance between the two classes to be predicted, which is the case for both distributions analyzed in our study (compare <xref rid="diagnostics-13-02315-t001" ref-type="table">Table 1</xref> and <xref rid="diagnostics-13-02315-t002" ref-type="table">Table 2</xref>). One approach to dealing with imbalanced data is to use so-called oversampling and undersampling techniques such as SMOTE (synthetic minority over-sampling technique) [<xref rid="B26-diagnostics-13-02315" ref-type="bibr">26</xref>,<xref rid="B27-diagnostics-13-02315" ref-type="bibr">27</xref>] and ROSE (random over-sampling examples) [<xref rid="B28-diagnostics-13-02315" ref-type="bibr">28</xref>,<xref rid="B29-diagnostics-13-02315" ref-type="bibr">29</xref>]. These techniques are of particular interest in classification problems where one class is significantly less frequently occupied compared to the other class(es). A simple way to correct such an imbalance is to remove records assigned to the more populated class(es) from the data or, alternatively, duplicate records assigned to the less populated class. Similar options are also available for H2O AutoML. Here, the &#x0201c;balance_classes&#x0201d; option can be used to balance class distributions. When enabled, H2O either undersamples the majority classes or oversamples the minority classes [<xref rid="B24-diagnostics-13-02315" ref-type="bibr">24</xref>]. We tested these techniques extensively in our study, but they did not significantly change our results. Therefore, to ensure that the methodological approach used for our conventional machine learning models and for AutoML is as comparable and simple as possible, we decided not to use such oversampling and undersampling techniques in our study. In our study, we consider the area under the precision-recall curve (AUPRC) instead of accuracy, sensitivity, or specificity. This is a metric that is particularly suitable for unbalanced data. However, it should be noted that AutoML offers many possibilities to optimize the confusion matrix (which summarizes the classification results) in terms of accuracy, sensitivity, specificity, and numerous other metrics such as precision, recall, the F1 score, and Matthew&#x02019;s correlation coefficient (MCC).</p><p>When running AutoML, numerous different algorithms are trained simultaneously. Therefore, the best of these models must subsequently be determined using a suitable metric (the so-called &#x0201c;sort metric&#x0201d;). We tested different metrics (AUC, AUPRC, log loss, mean per class error) and then decided to use AUPRC to account for the imbalance in our data. It should be noted, however, that the influence of the sorting metric used on the achieved discriminatory power of our final models was rather small, at least with respect to the sorting metrics we tested.</p><p>For the sake of clarity, we have summarized the most important methodological points and algorithms used in our study once again in <xref rid="diagnostics-13-02315-t003" ref-type="table">Table 3</xref>. The table also lists all the conventional machine-learning algorithms we tested. As described, all steps listed in <xref rid="diagnostics-13-02315-t003" ref-type="table">Table 3</xref> are performed 100 times for each model tested. A new data partition is used in each cycle. The independent test data is used only to determine model performance. The final model performance is determined as the average of these 100 runs.</p></sec></sec><sec sec-type="results" id="sec3-diagnostics-13-02315"><title>3. Results</title><sec id="sec3dot1-diagnostics-13-02315"><title>3.1. Finding the Best Class of Model Algorithms with AutoML</title><p>At the beginning of our analyses with AutoML, we initially used all classes of model algorithms (i.e., &#x0201c;GLM&#x0201d;, &#x0201c;DRF&#x0201d;, &#x0201c;GBM&#x0201d;, &#x0201c;DeepLearning&#x0201d;, and &#x0201c;StackedEnsemble&#x0201d;) for model training with the exception of the XGBoost GBM models. For both IDH mutation status prediction and ATRX status prediction, we initially performed 100 runs each with different splits of the data into training and test data. In the case of IDH mutation status prediction, AutoML mainly chose GBM algorithms. For the prediction of the ATRX status, on the other hand, mainly deep-learning algorithms were selected. This selection was also only subordinately dependent on the sorting metric used to determine the best model algorithm in each case. However, during these initial trials, we quickly realized that the algorithm classes selected by AutoML based on the training data (i.e., &#x0201c;GBM&#x0201d; and &#x0201c;DeepLearning&#x0201d;) did not necessarily also provide the best results in terms of the associated independent test data. The deep-learning algorithms and the stacked ensembles even performed the worst of all algorithms on both optimization problems in terms of the performance achieved with the independent test data. The fact that the deep-learning algorithms performed rather poorly in our experiments is certainly partly due to the comparatively small number of data sets included in our study cohort. However, this indicates that AutoML does not necessarily find the most suitable algorithm class for a particular optimization problem on its own.</p><p>Therefore, in a second experiment, we fixed each of the possible algorithm classes once, then performed again 100 runs each, and finally compared the results with respect to the independent test data. This approach has the advantage that it can also be used to test algorithms that AutoML would otherwise not or only rarely select. Conversely, this approach is also suitable for quickly excluding classes of algorithms (such as &#x0201c;DeepLearning&#x0201d; in our case) that may even require long optimization times but may prove to be suboptimal when subsequently tested with independent data. In this way, the computing time invested can be used to optimize the truly promising algorithms.</p><p>Applying the described approach to determine the most promising class of algorithms, we found in the case of IDH mutation status prediction that the highest performance using the independent test data was indeed achieved with GBM algorithms. As described, in the case where no specifications were made, AutoML also selected this algorithm class most frequently on its own. Without specifications, AutoML has also selected the algorithm classes &#x0201c;DRF&#x0201d;, &#x0201c;DeepLearning&#x0201d;, and &#x0201c;StackedEnsemble&#x0201d; in rare cases. However, our tests also showed that models developed with GLM algorithms performed only slightly worse than models developed with GBM algorithms. This is interesting in the sense that AutoML practically never automatically selected this class of algorithms.</p><p>Next, we determined the most promising class of algorithms for predicting ATRX status. As already described, AutoML without specifications selected deep-learning algorithms almost without exception, but these performed particularly poorly on the dependent test data. However, when testing all classes of algorithms, we quickly found that the GLM algorithms were the most promising for predicting ATRX status. This is remarkable because, in our very elaborate search for the most suitable conventional ML algorithm, we finally found a Lasso regression. Lasso regression (together with other algorithms such as ridge regression or an elastic net) belongs to the class of GLM algorithms. Using AutoML, we identified the class of the most suitable algorithms in the shortest possible time and thus saved a lot of time that we had previously spent searching for a suitable conventional ML algorithm.</p></sec><sec id="sec3dot2-diagnostics-13-02315"><title>3.2. Prediction of IDH Mutation Status Using AutoML</title><p>After having found the most promising class of algorithms for both IDH mutation status prediction and ATRX status prediction, we tested AutoML in combination with the three feature preselection options already described: (1) no feature preselection, i.e., using all features; (2) feature preselection using Lasso regression to determine the most important features; and (3) RFE. In our further analyses, we first considered the IDH mutation status again. We developed corresponding models with AutoML using all three methods for possible feature preselection and then compared the results with the results we were able to achieve with our best conventional machine learning algorithm.</p><p>The results of our calculations for the prediction of IDH mutation status are summarized in <xref rid="diagnostics-13-02315-f003" ref-type="fig">Figure 3</xref>. All performance values were calculated with independent test data and as mean values of 100 runs each. The left figure shows the results in terms of AUC achieved, and the right figure in terms of AUPRC. The exact numerical values are listed in <xref rid="diagnostics-13-02315-t004" ref-type="table">Table 4</xref>, together with their 95% confidence intervals. The values given in the table for the feature preselection by means of Lasso regression refer to the model with 10 features included. This number of features resulted in the highest discriminatory power. The RFE algorithm, on the other hand, selected slightly more features, with an average of 19.5 features. The third model without a feature preselection algorithm contained 63 features. Finally, the best conventional ML model resulted in 15 features included.</p><p>It is very interesting to observe that all three AutoML approaches produced extremely similar results compared to our best conventional ML algorithm, which we had elaborately developed beforehand. However, using AutoML, we needed significantly less time, and nevertheless, we achieved the same discriminatory power. Overall, the AutoML approach without a preceding feature preselection algorithm performed even better. Using independent test data, this approach yielded a mean AUC of 0.7400 [0.5041, 0.9326] and a mean AUPRC of 0.8582 [0.6737, 0.9695] on average (100 cycles), demonstrating high performance in discriminating IDH wild-type from IDH mutated gliomas. The numbers in brackets indicate the 95% confidence interval. Since no feature preselection was performed in this approach, this methodology is particularly simple and, at the same time, extremely fast in terms of the computing time required. We have limited the time to train/optimize each of the AutoML GBM models to a few minutes using the &#x0201c;max_runtime_secs&#x0201d; parameter. In our case, however, we even found that 60 s was sufficient to achieve comparable results and that longer training times did not contribute significantly to a further improvement in discriminatory power.</p><p>It should be noted here that we also calculated <italic toggle="yes">p</italic>-values for our final models. For this purpose, we compared our models to a null model using the ANOVA function in R (ANOVA = analysis of variance). By construction, the <italic toggle="yes">p</italic>-values obtained depended slightly on the individual data partitioning during each of the 100 cycles. However, the <italic toggle="yes">p</italic>-values were always smaller than 10 &#x000d7; 10<sup>&#x02212;6</sup>. We also compared the four final models listed in <xref rid="diagnostics-13-02315-t004" ref-type="table">Table 4</xref> using the DeLong test. No single comparison of the two models showed a significant difference (significance level <italic toggle="yes">p</italic> &#x0003c; 0.05) in terms of the AUCs achieved. However, when comparing the four models with a null model using the DeLong test, again only extremely small <italic toggle="yes">p</italic>-values (<italic toggle="yes">p</italic> &#x0003c; 10 &#x000d7; 10<sup>&#x02212;6</sup>) were obtained for all four models. Therefore, the likelihood that our final models do not significantly differentiate the two groups of IDH mutation status is extremely low.</p><p>In addition to the results already shown and discussed, we calculated the AUC, AUPRC, and several other performance values for our best AutoML model (AutoML without feature preselection) based on all 124 datasets used. We used 5-fold cross-validation. With respect to the validation data, we obtained the following performance results: AUC = 0.7832 [0.0743], AUPRC = 0.8870 [0.0459], accuracy = 0.7740 [0.0733], balanced accuracy = 0.6965 [0.1159], MCC = 0.5617 [0.0947], and F1 score = 0.8352 [0.0605]. The values in brackets indicate the standard deviation of the 5 validation groups in the 5-fold cross-validation. The comparison of performance in terms of AUC and AUPRC shows (see <xref rid="diagnostics-13-02315-t004" ref-type="table">Table 4</xref>) that very similar results were obtained with the independent test data as with the validation data. As expected, a slightly higher discriminatory power is achieved with the validation data compared to the independent test data.</p></sec><sec id="sec3dot3-diagnostics-13-02315"><title>3.3. Prediction of ATRX Status Using AutoML</title><p>Consistent with the prediction of IDH mutation status, we also examined the performance achieved with AutoML in predicting ATRX status. As already described, this time GLM algorithms very quickly proved to be the most promising algorithm class for model development with AutoML. Among the conventional ML algorithms tested, Lasso regression also turned out to be the most suitable algorithm for predicting ATRX status.</p><p>After deciding on GLM algorithms for training the AutoML models, we again ran the three different feature preselection procedures. AutoML took only a few seconds to train/optimize a GLM model (this time without setting a maximum calculation time), so despite our 100 repetitions, we were able to calculate the entire model set of 1700 individual models ((15 &#x000d7; Lasso feature preselection resulting in 1 to 15 features + 1 &#x000d7; RFE + 1 &#x000d7; no feature preselection) &#x000d7; 100 = 1700) in only a few hours. In fact, most of the computing time was even spent on feature preselection using Lasso regression or the RFE algorithm, which was performed before the subsequent model optimization with AutoML.</p><p><xref rid="diagnostics-13-02315-f004" ref-type="fig">Figure 4</xref> shows the results obtained (again, averaged over 100 runs). In addition, <xref rid="diagnostics-13-02315-t005" ref-type="table">Table 5</xref> summarizes the exact numerical performance values together with their confidence intervals. Once again, all four model approaches tested led to extremely similar performance values. This time, the best results overall were achieved with AutoML in combination with Lasso regression for feature preselection. The highest discriminatory power was achieved here with 13 features included, resulting in a mean AUC of 0.7810 [0.5563, 0.9111] and a mean AUPRC of 0.8511 [0.6603, 0.9522]. The results demonstrate that ATRX status can also be predicted with very high discriminatory power. The number of features in this model corresponds exactly to the number of features determined for the best conventional ML model. This is probably due to the fact that in the case of ATRX status prediction, both approaches (AutoML + conventional ML) used very similar algorithms (i.e., GLMs). In the case where RFE was used for feature preselection, 36.2 features were selected on average. In total (without feature preselection), 63 features were again available. However, in line with the prediction of the IDH mutation status, the method of feature preselection used again had only a minor influence on the discriminatory power achieved.</p><p>We also calculated <italic toggle="yes">p</italic>-values for our models predicting ATRX status. Consistent with the prediction of IDH mutation status, the <italic toggle="yes">p</italic>-values of the final models predicting ATRX status were less than 10 &#x000d7; 10<sup>&#x02212;6</sup>. The pairwise comparison of the four models listed in <xref rid="diagnostics-13-02315-t005" ref-type="table">Table 5</xref> using the DeLong test again showed no significant differences with regard to the AUCs achieved. The biggest difference was found when comparing the two AutoML models where no feature preselection or feature preselection using Lasso regression was performed. However, when comparing the four models with a null model, all four <italic toggle="yes">p</italic>-values again indicated high significance (<italic toggle="yes">p</italic>-values &#x0003c; 10 &#x000d7; 10<sup>&#x02212;6</sup>). Finally, for our best AutoML model for predicting ATRX status (feature preselection with Lasso regression and 13 features included), we also calculated the AUC, AUPRC, and several other performance values based on all 124 datasets used. Consistent with the approach used to predict IDH mutation status, we again used 5-fold cross-validation. For the validation data, we obtained the following performance results: AUC = 0.8268 [0.0680], AUPRC = 0.8703 [0.0559], accuracy = 0.8230 [0.0659], balanced accuracy = 0.8099 [0.0760], MCC = 0.6316 [0.1400], and F1 score = 0.8594 [0.0505]. The values in brackets again indicate the standard deviation of the 5 validation groups in the 5-fold cross-validation. The comparison of performance in terms of AUC and AUPRC shows (see <xref rid="diagnostics-13-02315-t005" ref-type="table">Table 5</xref>) that again very similar results were obtained with the independent test data compared to the validation data.</p></sec></sec><sec sec-type="discussion" id="sec4-diagnostics-13-02315"><title>4. Discussion</title><p>In this study, we investigated the added value of automated machine learning for potential applications in medical cancer diagnostics. Based on two important diagnostic questions, the prediction of IDH mutation status and ATRX status, we found that it is already possible to develop models using AutoML that have comparable discriminatory power to conventional ML models developed by technically experienced developers. Using AutoML, we were able to significantly reduce the time required to develop these two models compared to conventional approaches and still achieve the same discriminatory power. Furthermore, we believe that the significant simplifications associated with AutoML in the development of ML models already enable non-experts to develop powerful models. The AutoML models we developed required only a few lines of computer code. Numerous algorithms were tested simultaneously in a very short period of time. This is where we currently see the greatest added value of AutoML. We were able to quickly identify promising classes of model algorithms for further model development. This step is often very time-consuming in conventional machine learning. Using AutoML, we were able to concentrate directly on fine-tuning the models. Until now, testing possible conventional ML algorithms required a considerable amount of expert knowledge, e.g., for the appropriate determination of the hyperparameters included in the various model algorithms. However, algorithms such as AutoML represent an essential step towards better meeting the significant demand for ML algorithms for future medical applications. Some other studies have already found comparable evaluations to ours. For example, Antaki et al. assessed the discriminative performance of AutoML in differentiating retinal vein occlusion (RVO), retinitis pigmentosa (RP), and retinal detachment (RD) from normal fundi using ultra-widefield pseudocolor fundus images. They used a publicly available image data set of 2137 labeled images, reviewed the data set for low-quality and mislabeled images, and then uploaded them to the Google Cloud AutoML Vision platform for training and testing. The performance achieved was comparable to deep-learning models derived by AI experts for RVO and RP but not for RD [<xref rid="B30-diagnostics-13-02315" ref-type="bibr">30</xref>]. Abbas et al. evaluated the performance of an AutoML model that predicts visual acuity outcomes in patients receiving treatment for neovascular age-related macular degeneration, in comparison to a manually coded model built using the same dataset. Their AutoML model achieved a highly similar performance to their XGBoost model (AUC-AutoML: 0.849, AUC-XGBoost: 0.847) [<xref rid="B31-diagnostics-13-02315" ref-type="bibr">31</xref>]. Romero et al. compared the performance of different AutoML tools for predicting outcomes for different diseases of interest on datasets with high class imbalance. The AutoML tools showed improvement from the baseline random forest model but did not differ significantly from each other [<xref rid="B32-diagnostics-13-02315" ref-type="bibr">32</xref>]. A further comparison of an AutoML algorithm with conventional algorithms was also carried out by Ou et al. using the example of the predictability of intracranial aneurysm treatment outcomes [<xref rid="B7-diagnostics-13-02315" ref-type="bibr">7</xref>]. They compared the AutoML algorithm with multivariate regression and a random forest model. In this study, the AutoML model even performed significantly better than the two conventional models.</p><p>In our study, in addition to testing AutoML, we also analyzed different options for feature preselecting. Interestingly, for both diagnostic questions examined in the study, we found that the feature preselection methodology used had no significant influence on the discriminatory power achieved. We even achieved similar discriminatory power with and without feature preselection. However, there are some important points to note here. First, it is important to mention that, as described, we subjected all features to a 95% correlation filter before training the models to account for redundancies between features. Without the use of the correlation filter, the models proved to be unstable and showed lower discriminatory power. Secondly, it should also be noted that AutoML does not actually use all the features offered for training in the final models. In the case of GLM models, for example, the number of effectively used features is limited by regularization techniques. Thirdly, we trained our models with only 100 data sets each and a maximum of 63 possible features. However, if, for example, databases with considerably more data sets and features are available, algorithms for feature preselection are of great importance, as otherwise, very long computing times may become necessary very quickly. Overall, the fact that feature preselection, with the exception of the correlation filter used, was not beneficial in our two optimization problems studied should therefore not be generalized.</p><p>Regardless of the added value of H2O AutoML specifically investigated in this study, it is generally important in automated machine learning to find an appropriate combination of feature preprocessing, feature preselection, model selection, and hyperparameter tuning. Due to the many factors to be taken into account, this task can quickly become very complex and computationally intensive. The combinations of several of these individual steps are also called &#x0201c;pipelines&#x0201d;. Accordingly, so-called &#x0201c;pipeline optimizers&#x0201d; are systems that support the automation of several machine learning steps. Two of these optimization algorithms are TPOT (tree-based pipeline optimization Tool) [<xref rid="B33-diagnostics-13-02315" ref-type="bibr">33</xref>] and Auto-Sklearn [<xref rid="B34-diagnostics-13-02315" ref-type="bibr">34</xref>]. As an example of the application of such techniques, Dafflon et al. used TPOT to conduct a genetic programming-based approach to find a model and its hyperparameters that more closely predicts the brain age from cortical anatomical measures [<xref rid="B35-diagnostics-13-02315" ref-type="bibr">35</xref>].</p><p>Our study has some limitations. First, the retrospective character and the fact that it is a single-center study should be mentioned. In addition, we had to exclude 12 patients due to unknown IDH mutation status and/or ATRX status. Finally, the data set we used included only 124 patients. To obtain the most reliable results, we have therefore fully developed each of our models 100 times and then determined the performance and its variability (confidence intervals) with the corresponding independent test data. However, to be able to analyze the performance of AutoML even more precisely, it would be advantageous to relate corresponding analyses to even larger study cohorts. Regardless of these limitations, this study shows that AutoML algorithms can significantly simplify and automate numerous steps in the model development of conventional ML algorithms, making such methods also accessible to non-experts. But AutoML also offers considerable advantages for experts, as these techniques can be used to develop high-performance models in much less time than before.</p></sec><sec sec-type="conclusions" id="sec5-diagnostics-13-02315"><title>5. Conclusions</title><p>The demand for artificial intelligence and machine learning techniques for potential medical applications is increasing rapidly. In order to meet this demand as effectively as possible, it is important to simplify these techniques to make them also accessible to non-experts. Such a possible and very promising considerable simplification can be achieved by means of automated machine learning algorithms. We have investigated such an algorithm using two clinical case studies: the prediction of IDH mutation status and ATRX status, which are two very important markers in cancer diagnostics. With only a few lines of computer code, we were able to develop models in a minimum amount of time that nevertheless have comparable discriminatory power to conventional machine learning models but whose development required a considerable amount of expert knowledge.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, M.M. (Manfred Musigmann); Validation, W.H.; Investigation, N.G.N. and D.N.K.; Supervision, M.M. (Manoj Mannil). All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>The study was conducted in accordance with the Declaration of Helsinki, and approved by the local ethics committee &#x000c4;rztekammer Westfalen Lippe and University of M&#x000fc;nster (protocol code 2021-596-f-S).</p></notes><notes><title>Informed Consent Statement</title><p>Informed consent was waived.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Available upon reasonable request.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>No conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-diagnostics-13-02315"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Mesk&#x000f3;</surname><given-names>B.</given-names></name>
<name><surname>G&#x000f6;r&#x000f6;g</surname><given-names>M.</given-names></name>
</person-group><article-title>A Short Guide for Medical Professionals in the Era of Artificial Intelligence</article-title><source>NPJ Digit. Med.</source><year>2020</year><volume>3</volume><fpage>126</fpage><pub-id pub-id-type="doi">10.1038/s41746-020-00333-z</pub-id><pub-id pub-id-type="pmid">33043150</pub-id></element-citation></ref><ref id="B2-diagnostics-13-02315"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ari</surname><given-names>A.P.</given-names></name>
<name><surname>Akkurt</surname><given-names>B.H.</given-names></name>
<name><surname>Musigmann</surname><given-names>M.</given-names></name>
<name><surname>Mammadov</surname><given-names>O.</given-names></name>
<name><surname>Bl&#x000f6;mer</surname><given-names>D.A.</given-names></name>
<name><surname>Kasap</surname><given-names>D.N.G.</given-names></name>
<name><surname>Henssen</surname><given-names>D.J.H.A.</given-names></name>
<name><surname>Nacul</surname><given-names>N.G.</given-names></name>
<name><surname>Sartoretti</surname><given-names>E.</given-names></name>
<name><surname>Sartoretti</surname><given-names>T.</given-names></name>
<etal/>
</person-group><article-title>Pseudoprogression Prediction in High Grade Primary CNS Tumors by Use of Radiomics</article-title><source>Sci. Rep.</source><year>2022</year><volume>12</volume><fpage>5915</fpage><pub-id pub-id-type="doi">10.1038/s41598-022-09945-9</pub-id><pub-id pub-id-type="pmid">35396525</pub-id></element-citation></ref><ref id="B3-diagnostics-13-02315"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kr&#x000e4;hling</surname><given-names>H.</given-names></name>
<name><surname>Musigmann</surname><given-names>M.</given-names></name>
<name><surname>Akkurt</surname><given-names>B.H.</given-names></name>
<name><surname>Sartoretti</surname><given-names>T.</given-names></name>
<name><surname>Sartoretti</surname><given-names>E.</given-names></name>
<name><surname>Henssen</surname><given-names>D.J.H.A.</given-names></name>
<name><surname>Stummer</surname><given-names>W.</given-names></name>
<name><surname>Heindel</surname><given-names>W.</given-names></name>
<name><surname>Brokinkel</surname><given-names>B.</given-names></name>
<name><surname>Mannil</surname><given-names>M.</given-names></name>
</person-group><article-title>A Magnetic Resonance Imaging Based Radiomics Model to Predict Mitosis Cycles in Intracranial Meningioma</article-title><source>Sci. Rep.</source><year>2023</year><volume>13</volume><fpage>969</fpage><pub-id pub-id-type="doi">10.1038/s41598-023-28089-y</pub-id><pub-id pub-id-type="pmid">36653482</pub-id></element-citation></ref><ref id="B4-diagnostics-13-02315"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Musigmann</surname><given-names>M.</given-names></name>
<name><surname>Akkurt</surname><given-names>B.H.</given-names></name>
<name><surname>Kr&#x000e4;hling</surname><given-names>H.</given-names></name>
<name><surname>Brokinkel</surname><given-names>B.</given-names></name>
<name><surname>Henssen</surname><given-names>D.J.H.A.</given-names></name>
<name><surname>Sartoretti</surname><given-names>T.</given-names></name>
<name><surname>Nacul</surname><given-names>N.G.</given-names></name>
<name><surname>Stummer</surname><given-names>W.</given-names></name>
<name><surname>Heindel</surname><given-names>W.</given-names></name>
<name><surname>Mannil</surname><given-names>M.</given-names></name>
</person-group><article-title>Assessing Preoperative Risk of STR in Skull Meningiomas Using MR Radiomics and Machine Learning</article-title><source>Sci. Rep.</source><year>2022</year><volume>12</volume><fpage>14043</fpage><pub-id pub-id-type="doi">10.1038/s41598-022-18458-4</pub-id><pub-id pub-id-type="pmid">35982218</pub-id></element-citation></ref><ref id="B5-diagnostics-13-02315"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ikemura</surname><given-names>K.</given-names></name>
<name><surname>Bellin</surname><given-names>E.</given-names></name>
<name><surname>Yagi</surname><given-names>Y.</given-names></name>
<name><surname>Billett</surname><given-names>H.</given-names></name>
<name><surname>Saada</surname><given-names>M.</given-names></name>
<name><surname>Simone</surname><given-names>K.</given-names></name>
<name><surname>Stahl</surname><given-names>L.</given-names></name>
<name><surname>Szymanski</surname><given-names>J.</given-names></name>
<name><surname>Goldstein</surname><given-names>D.Y.</given-names></name>
<name><surname>Reyes Gil</surname><given-names>M.</given-names></name>
</person-group><article-title>Using Automated Machine Learning to Predict the Mortality of Patients with COVID-19: Prediction Model Development Study</article-title><source>J. Med. Internet Res.</source><year>2021</year><volume>23</volume><fpage>e23458</fpage><pub-id pub-id-type="doi">10.2196/23458</pub-id><pub-id pub-id-type="pmid">33539308</pub-id></element-citation></ref><ref id="B6-diagnostics-13-02315"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Karaglani</surname><given-names>M.</given-names></name>
<name><surname>Gourlia</surname><given-names>K.</given-names></name>
<name><surname>Tsamardinos</surname><given-names>I.</given-names></name>
<name><surname>Chatzaki</surname><given-names>E.</given-names></name>
</person-group><article-title>Accurate Blood-Based Diagnostic Biosignatures for Alzheimer&#x02019;s Disease via Automated Machine Learning</article-title><source>J. Clin. Med.</source><year>2020</year><volume>9</volume><elocation-id>3016</elocation-id><pub-id pub-id-type="doi">10.3390/jcm9093016</pub-id><pub-id pub-id-type="pmid">32962113</pub-id></element-citation></ref><ref id="B7-diagnostics-13-02315"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ou</surname><given-names>C.</given-names></name>
<name><surname>Liu</surname><given-names>J.</given-names></name>
<name><surname>Qian</surname><given-names>Y.</given-names></name>
<name><surname>Chong</surname><given-names>W.</given-names></name>
<name><surname>Liu</surname><given-names>D.</given-names></name>
<name><surname>He</surname><given-names>X.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Duan</surname><given-names>C.-Z.</given-names></name>
</person-group><article-title>Automated Machine Learning Model Development for Intracranial Aneurysm Treatment Outcome Prediction: A Feasibility Study</article-title><source>Front. Neurol.</source><year>2021</year><volume>12</volume><fpage>735142</fpage><pub-id pub-id-type="doi">10.3389/fneur.2021.735142</pub-id><pub-id pub-id-type="pmid">34912282</pub-id></element-citation></ref><ref id="B8-diagnostics-13-02315"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Touma</surname><given-names>S.</given-names></name>
<name><surname>Antaki</surname><given-names>F.</given-names></name>
<name><surname>Duval</surname><given-names>R.</given-names></name>
</person-group><article-title>Development of a Code-Free Machine Learning Model for the Classification of Cataract Surgery Phases</article-title><source>Sci. Rep.</source><year>2022</year><volume>12</volume><fpage>2398</fpage><pub-id pub-id-type="doi">10.1038/s41598-022-06127-5</pub-id><?supplied-pmid 35165304?><pub-id pub-id-type="pmid">35165304</pub-id></element-citation></ref><ref id="B9-diagnostics-13-02315"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Waring</surname><given-names>J.</given-names></name>
<name><surname>Lindvall</surname><given-names>C.</given-names></name>
<name><surname>Umeton</surname><given-names>R.</given-names></name>
</person-group><article-title>Automated Machine Learning: Review of the State-of-the-Art and Opportunities for Healthcare</article-title><source>Artif. Intell. Med.</source><year>2020</year><volume>104</volume><fpage>101822</fpage><pub-id pub-id-type="doi">10.1016/j.artmed.2020.101822</pub-id><?supplied-pmid 32499001?><pub-id pub-id-type="pmid">32499001</pub-id></element-citation></ref><ref id="B10-diagnostics-13-02315"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Luo</surname><given-names>G.</given-names></name>
</person-group><article-title>A Review of Automatic Selection Methods for Machine Learning Algorithms and Hyper-Parameter Values</article-title><source>Netw. Model. Anal. Health Inform. Bioinform.</source><year>2016</year><volume>5</volume><fpage>18</fpage><pub-id pub-id-type="doi">10.1007/s13721-016-0125-6</pub-id></element-citation></ref><ref id="B11-diagnostics-13-02315"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>B.</given-names></name>
<name><surname>Chen</surname><given-names>C.</given-names></name>
<name><surname>Wang</surname><given-names>J.</given-names></name>
<name><surname>Teng</surname><given-names>Y.</given-names></name>
<name><surname>Ma</surname><given-names>X.</given-names></name>
<name><surname>Xu</surname><given-names>J.</given-names></name>
</person-group><article-title>Differentiation of Low-Grade Astrocytoma From Anaplastic Astrocytoma Using Radiomics-Based Machine Learning Techniques</article-title><source>Front. Oncol.</source><year>2021</year><volume>11</volume><fpage>521313</fpage><pub-id pub-id-type="doi">10.3389/fonc.2021.521313</pub-id><?supplied-pmid 34141605?><pub-id pub-id-type="pmid">34141605</pub-id></element-citation></ref><ref id="B12-diagnostics-13-02315"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Park</surname><given-names>Y.W.</given-names></name>
<name><surname>Choi</surname><given-names>Y.S.</given-names></name>
<name><surname>Ahn</surname><given-names>S.S.</given-names></name>
<name><surname>Chang</surname><given-names>J.H.</given-names></name>
<name><surname>Kim</surname><given-names>S.H.</given-names></name>
<name><surname>Lee</surname><given-names>S.K.</given-names></name>
</person-group><article-title>Radiomics MRI Phenotyping with Machine Learning to Predict the Grade of Lower-Grade Gliomas: A Study Focused on Nonenhancing Tumors</article-title><source>Korean J. Radiol.</source><year>2019</year><volume>20</volume><fpage>1381</fpage><lpage>1389</lpage><pub-id pub-id-type="doi">10.3348/kjr.2018.0814</pub-id><?supplied-pmid 31464116?><pub-id pub-id-type="pmid">31464116</pub-id></element-citation></ref><ref id="B13-diagnostics-13-02315"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Citak-Er</surname><given-names>F.</given-names></name>
<name><surname>Firat</surname><given-names>Z.</given-names></name>
<name><surname>Kovanlikaya</surname><given-names>I.</given-names></name>
<name><surname>Ture</surname><given-names>U.</given-names></name>
<name><surname>Ozturk-Isik</surname><given-names>E.</given-names></name>
</person-group><article-title>Machine-Learning in Grading of Gliomas Based on Multi-Parametric Magnetic Resonance Imaging at 3T</article-title><source>Comput. Biol. Med.</source><year>2018</year><volume>99</volume><fpage>154</fpage><lpage>160</lpage><pub-id pub-id-type="doi">10.1016/j.compbiomed.2018.06.009</pub-id><pub-id pub-id-type="pmid">29933126</pub-id></element-citation></ref><ref id="B14-diagnostics-13-02315"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gao</surname><given-names>M.</given-names></name>
<name><surname>Huang</surname><given-names>S.</given-names></name>
<name><surname>Pan</surname><given-names>X.</given-names></name>
<name><surname>Liao</surname><given-names>X.</given-names></name>
<name><surname>Yang</surname><given-names>R.</given-names></name>
<name><surname>Liu</surname><given-names>J.</given-names></name>
</person-group><article-title>Machine Learning-Based Radiomics Predicting Tumor Grades and Expression of Multiple Pathologic Biomarkers in Gliomas</article-title><source>Front. Oncol.</source><year>2020</year><volume>10</volume><fpage>1676</fpage><pub-id pub-id-type="doi">10.3389/fonc.2020.01676</pub-id><?supplied-pmid 33014836?><pub-id pub-id-type="pmid">33014836</pub-id></element-citation></ref><ref id="B15-diagnostics-13-02315"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Yan</surname><given-names>L.-F.</given-names></name>
<name><surname>Hu</surname><given-names>Y.-C.</given-names></name>
<name><surname>Li</surname><given-names>G.</given-names></name>
<name><surname>Yang</surname><given-names>Y.</given-names></name>
<name><surname>Han</surname><given-names>Y.</given-names></name>
<name><surname>Sun</surname><given-names>Y.-Z.</given-names></name>
<name><surname>Liu</surname><given-names>Z.-C.</given-names></name>
<name><surname>Tian</surname><given-names>Q.</given-names></name>
<name><surname>Han</surname><given-names>Z.-Y.</given-names></name>
<etal/>
</person-group><article-title>Optimizing a Machine Learning Based Glioma Grading System Using Multi-Parametric MRI Histogram and Texture Features</article-title><source>Oncotarget</source><year>2017</year><volume>8</volume><fpage>47816</fpage><lpage>47830</lpage><pub-id pub-id-type="doi">10.18632/oncotarget.18001</pub-id><pub-id pub-id-type="pmid">28599282</pub-id></element-citation></ref><ref id="B16-diagnostics-13-02315"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhao</surname><given-names>S.-S.</given-names></name>
<name><surname>Feng</surname><given-names>X.-L.</given-names></name>
<name><surname>Hu</surname><given-names>Y.-C.</given-names></name>
<name><surname>Han</surname><given-names>Y.</given-names></name>
<name><surname>Tian</surname><given-names>Q.</given-names></name>
<name><surname>Sun</surname><given-names>Y.-Z.</given-names></name>
<name><surname>Zhang</surname><given-names>J.</given-names></name>
<name><surname>Ge</surname><given-names>X.-W.</given-names></name>
<name><surname>Cheng</surname><given-names>S.-C.</given-names></name>
<name><surname>Li</surname><given-names>X.-L.</given-names></name>
<etal/>
</person-group><article-title>Better Efficacy in Differentiating WHO Grade II from III Oligodendrogliomas with Machine-Learning than Radiologist&#x02019;s Reading from Conventional T1 Contrast-Enhanced and Fluid Attenuated Inversion Recovery Images</article-title><source>BMC Neurol.</source><year>2020</year><volume>20</volume><elocation-id>48</elocation-id><pub-id pub-id-type="doi">10.1186/s12883-020-1613-y</pub-id><pub-id pub-id-type="pmid">32033580</pub-id></element-citation></ref><ref id="B17-diagnostics-13-02315"><label>17.</label><element-citation publication-type="webpage"><article-title>IARC Publications Website&#x02014;Central Nervous System Tumours</article-title><comment>Available online: <ext-link xlink:href="https://publications.iarc.fr/Book-And-Report-Series/Who-Classification-Of-Tumours/Central-Nervous-System-Tumours-2021" ext-link-type="uri">https://publications.iarc.fr/Book-And-Report-Series/Who-Classification-Of-Tumours/Central-Nervous-System-Tumours-2021</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2022-10-02">(accessed on 2 October 2022)</date-in-citation></element-citation></ref><ref id="B18-diagnostics-13-02315"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Louis</surname><given-names>D.N.</given-names></name>
<name><surname>Perry</surname><given-names>A.</given-names></name>
<name><surname>Wesseling</surname><given-names>P.</given-names></name>
<name><surname>Brat</surname><given-names>D.J.</given-names></name>
<name><surname>Cree</surname><given-names>I.A.</given-names></name>
<name><surname>Figarella-Branger</surname><given-names>D.</given-names></name>
<name><surname>Hawkins</surname><given-names>C.</given-names></name>
<name><surname>Ng</surname><given-names>H.K.</given-names></name>
<name><surname>Pfister</surname><given-names>S.M.</given-names></name>
<name><surname>Reifenberger</surname><given-names>G.</given-names></name>
<etal/>
</person-group><article-title>The 2021 WHO Classification of Tumors of the Central Nervous System: A Summary</article-title><source>Neuro Oncol.</source><year>2021</year><volume>23</volume><fpage>1231</fpage><lpage>1251</lpage><pub-id pub-id-type="doi">10.1093/neuonc/noab106</pub-id><pub-id pub-id-type="pmid">34185076</pub-id></element-citation></ref><ref id="B19-diagnostics-13-02315"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wen</surname><given-names>P.Y.</given-names></name>
<name><surname>Packer</surname><given-names>R.J.</given-names></name>
</person-group><article-title>The 2021 WHO Classification of Tumors of the Central Nervous System: Clinical Implications</article-title><source>Neuro Oncol.</source><year>2021</year><volume>23</volume><fpage>1215</fpage><lpage>1217</lpage><pub-id pub-id-type="doi">10.1093/neuonc/noab120</pub-id><pub-id pub-id-type="pmid">34185090</pub-id></element-citation></ref><ref id="B20-diagnostics-13-02315"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Weller</surname><given-names>M.</given-names></name>
<name><surname>van den Bent</surname><given-names>M.</given-names></name>
<name><surname>Hopkins</surname><given-names>K.</given-names></name>
<name><surname>Tonn</surname><given-names>J.C.</given-names></name>
<name><surname>Stupp</surname><given-names>R.</given-names></name>
<name><surname>Falini</surname><given-names>A.</given-names></name>
<name><surname>Cohen-Jonathan-Moyal</surname><given-names>E.</given-names></name>
<name><surname>Frappaz</surname><given-names>D.</given-names></name>
<name><surname>Henriksson</surname><given-names>R.</given-names></name>
<name><surname>Balana</surname><given-names>C.</given-names></name>
<etal/>
</person-group><article-title>EANO Guideline for the Diagnosis and Treatment of Anaplastic Gliomas and Glioblastoma</article-title><source>Lancet Oncol.</source><year>2014</year><volume>15</volume><fpage>e395</fpage><lpage>e403</lpage><pub-id pub-id-type="doi">10.1016/S1470-2045(14)70011-7</pub-id><pub-id pub-id-type="pmid">25079102</pub-id></element-citation></ref><ref id="B21-diagnostics-13-02315"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yan</surname><given-names>H.</given-names></name>
<name><surname>Parsons</surname><given-names>D.W.</given-names></name>
<name><surname>Jin</surname><given-names>G.</given-names></name>
<name><surname>McLendon</surname><given-names>R.</given-names></name>
<name><surname>Rasheed</surname><given-names>B.A.</given-names></name>
<name><surname>Yuan</surname><given-names>W.</given-names></name>
<name><surname>Kos</surname><given-names>I.</given-names></name>
<name><surname>Batinic-Haberle</surname><given-names>I.</given-names></name>
<name><surname>Jones</surname><given-names>S.</given-names></name>
<name><surname>Riggins</surname><given-names>G.J.</given-names></name>
<etal/>
</person-group><article-title>IDH1 and IDH2 Mutations in Gliomas</article-title><source>N. Engl. J. Med.</source><year>2009</year><volume>360</volume><fpage>765</fpage><lpage>773</lpage><pub-id pub-id-type="doi">10.1056/NEJMoa0808710</pub-id><pub-id pub-id-type="pmid">19228619</pub-id></element-citation></ref><ref id="B22-diagnostics-13-02315"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>van Griethuysen</surname><given-names>J.J.M.</given-names></name>
<name><surname>Fedorov</surname><given-names>A.</given-names></name>
<name><surname>Parmar</surname><given-names>C.</given-names></name>
<name><surname>Hosny</surname><given-names>A.</given-names></name>
<name><surname>Aucoin</surname><given-names>N.</given-names></name>
<name><surname>Narayan</surname><given-names>V.</given-names></name>
<name><surname>Beets-Tan</surname><given-names>R.G.H.</given-names></name>
<name><surname>Fillion-Robin</surname><given-names>J.-C.</given-names></name>
<name><surname>Pieper</surname><given-names>S.</given-names></name>
<name><surname>Aerts</surname><given-names>H.J.W.L.</given-names></name>
</person-group><article-title>Computational Radiomics System to Decode the Radiographic Phenotype</article-title><source>Cancer Res.</source><year>2017</year><volume>77</volume><fpage>e104</fpage><lpage>e107</lpage><pub-id pub-id-type="doi">10.1158/0008-5472.CAN-17-0339</pub-id><pub-id pub-id-type="pmid">29092951</pub-id></element-citation></ref><ref id="B23-diagnostics-13-02315"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Darst</surname><given-names>B.F.</given-names></name>
<name><surname>Malecki</surname><given-names>K.C.</given-names></name>
<name><surname>Engelman</surname><given-names>C.D.</given-names></name>
</person-group><article-title>Using Recursive Feature Elimination in Random Forest to Account for Correlated Variables in High Dimensional Data</article-title><source>BMC Genet.</source><year>2018</year><volume>19</volume><elocation-id>65</elocation-id><pub-id pub-id-type="doi">10.1186/s12863-018-0633-8</pub-id><pub-id pub-id-type="pmid">30255764</pub-id></element-citation></ref><ref id="B24-diagnostics-13-02315"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>LeDell</surname><given-names>E.</given-names></name>
<name><surname>Poirier</surname><given-names>S.</given-names></name>
</person-group><article-title>H2O AutoML: Scalable Automatic Machine Learning</article-title><source>ICML</source><year>2020</year><volume>2020</volume><fpage>16</fpage></element-citation></ref><ref id="B25-diagnostics-13-02315"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Musigmann</surname><given-names>M.</given-names></name>
<name><surname>Akkurt</surname><given-names>B.H.</given-names></name>
<name><surname>Kr&#x000e4;hling</surname><given-names>H.</given-names></name>
<name><surname>Nacul</surname><given-names>N.G.</given-names></name>
<name><surname>Remonda</surname><given-names>L.</given-names></name>
<name><surname>Sartoretti</surname><given-names>T.</given-names></name>
<name><surname>Henssen</surname><given-names>D.</given-names></name>
<name><surname>Brokinkel</surname><given-names>B.</given-names></name>
<name><surname>Stummer</surname><given-names>W.</given-names></name>
<name><surname>Heindel</surname><given-names>W.</given-names></name>
<etal/>
</person-group><article-title>Testing the Applicability and Performance of Auto ML for Potential Applications in Diagnostic Neuroradiology</article-title><source>Sci. Rep.</source><year>2022</year><volume>12</volume><fpage>13648</fpage><pub-id pub-id-type="doi">10.1038/s41598-022-18028-8</pub-id><pub-id pub-id-type="pmid">35953588</pub-id></element-citation></ref><ref id="B26-diagnostics-13-02315"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chawla</surname><given-names>N.</given-names></name>
<name><surname>Bowyer</surname><given-names>K.</given-names></name>
<name><surname>Hall</surname><given-names>L.</given-names></name>
<name><surname>Kegelmeyer</surname><given-names>W.</given-names></name>
</person-group><article-title>SMOTE: Synthetic Minority Over-Sampling Technique</article-title><source>J. Artif. Intell. Res.</source><year>2002</year><volume>16</volume><fpage>321</fpage><lpage>357</lpage><pub-id pub-id-type="doi">10.1613/jair.953</pub-id></element-citation></ref><ref id="B27-diagnostics-13-02315"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Elreedy</surname><given-names>D.</given-names></name>
<name><surname>Atiya</surname><given-names>A.F.</given-names></name>
</person-group><article-title>A Comprehensive Analysis of Synthetic Minority Oversampling Technique (SMOTE) for Handling Class Imbalance</article-title><source>Inf. Sci.</source><year>2019</year><volume>505</volume><fpage>32</fpage><lpage>64</lpage><pub-id pub-id-type="doi">10.1016/j.ins.2019.07.070</pub-id></element-citation></ref><ref id="B28-diagnostics-13-02315"><label>28.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<name><surname>Lunardon</surname><given-names>N.</given-names></name>
<name><surname>Menardi</surname><given-names>G.</given-names></name>
<name><surname>Torelli</surname><given-names>N.</given-names></name>
<name><surname>Lunardon</surname><given-names>N.</given-names></name>
</person-group><article-title>ROSE: Random Over-Sampling Examples. 19</article-title><comment>Available online: <ext-link xlink:href="https://www.automl.org/wp-content/uploads/2020/07/AutoML_2020_paper_61.pdf" ext-link-type="uri">https://www.automl.org/wp-content/uploads/2020/07/AutoML_2020_paper_61.pdf</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2023-06-01">(accessed on 1 June 2023)</date-in-citation></element-citation></ref><ref id="B29-diagnostics-13-02315"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>J.</given-names></name>
<name><surname>Chen</surname><given-names>L.</given-names></name>
</person-group><article-title>Clustering-Based Undersampling with Random over Sampling Examples and Support Vector Machine for Imbalanced Classification of Breast Cancer Diagnosis</article-title><source>Comput. Assist. Surg.</source><year>2019</year><volume>24</volume><fpage>62</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1080/24699322.2019.1649074</pub-id></element-citation></ref><ref id="B30-diagnostics-13-02315"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Antaki</surname><given-names>F.</given-names></name>
<name><surname>Coussa</surname><given-names>R.G.</given-names></name>
<name><surname>Kahwati</surname><given-names>G.</given-names></name>
<name><surname>Hammamji</surname><given-names>K.</given-names></name>
<name><surname>Sebag</surname><given-names>M.</given-names></name>
<name><surname>Duval</surname><given-names>R.</given-names></name>
</person-group><article-title>Accuracy of Automated Machine Learning in Classifying Retinal Pathologies from Ultra-Widefield Pseudocolour Fundus Images</article-title><source>Br. J. Ophthalmol.</source><year>2023</year><volume>107</volume><fpage>90</fpage><lpage>95</lpage><pub-id pub-id-type="doi">10.1136/bjophthalmol-2021-319030</pub-id><?supplied-pmid 34344669?><pub-id pub-id-type="pmid">34344669</pub-id></element-citation></ref><ref id="B31-diagnostics-13-02315"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Abbas</surname><given-names>A.</given-names></name>
<name><surname>O&#x02019;Byrne</surname><given-names>C.</given-names></name>
<name><surname>Fu</surname><given-names>D.J.</given-names></name>
<name><surname>Moraes</surname><given-names>G.</given-names></name>
<name><surname>Balaskas</surname><given-names>K.</given-names></name>
<name><surname>Struyven</surname><given-names>R.</given-names></name>
<name><surname>Beqiri</surname><given-names>S.</given-names></name>
<name><surname>Wagner</surname><given-names>S.K.</given-names></name>
<name><surname>Korot</surname><given-names>E.</given-names></name>
<name><surname>Keane</surname><given-names>P.A.</given-names></name>
</person-group><article-title>Evaluating an Automated Machine Learning Model That Predicts Visual Acuity Outcomes in Patients with Neovascular Age-Related Macular Degeneration</article-title><source>Graefes Arch. Clin. Exp. Ophthalmol.</source><year>2022</year><volume>260</volume><fpage>2461</fpage><lpage>2473</lpage><pub-id pub-id-type="doi">10.1007/s00417-021-05544-y</pub-id><pub-id pub-id-type="pmid">35122132</pub-id></element-citation></ref><ref id="B32-diagnostics-13-02315"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Romero</surname><given-names>R.A.A.</given-names></name>
<name><surname>Deypalan</surname><given-names>M.N.Y.</given-names></name>
<name><surname>Mehrotra</surname><given-names>S.</given-names></name>
<name><surname>Jungao</surname><given-names>J.T.</given-names></name>
<name><surname>Sheils</surname><given-names>N.E.</given-names></name>
<name><surname>Manduchi</surname><given-names>E.</given-names></name>
<name><surname>Moore</surname><given-names>J.H.</given-names></name>
</person-group><article-title>Benchmarking AutoML Frameworks for Disease Prediction Using Medical Claims</article-title><source>BioData Min.</source><year>2022</year><volume>15</volume><fpage>15</fpage><pub-id pub-id-type="doi">10.1186/s13040-022-00300-2</pub-id><?supplied-pmid 35883154?><pub-id pub-id-type="pmid">35883154</pub-id></element-citation></ref><ref id="B33-diagnostics-13-02315"><label>33.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Olson</surname><given-names>R.S.</given-names></name>
<name><surname>Moore</surname><given-names>J.H.</given-names></name>
</person-group><article-title>TPOT: A Tree-Based Pipeline Optimization Tool for Automating Machine Learning</article-title><source>Automated Machine Learning: Methods, Systems, Challenges</source><person-group person-group-type="editor">
<name><surname>Hutter</surname><given-names>F.</given-names></name>
<name><surname>Kotthoff</surname><given-names>L.</given-names></name>
<name><surname>Vanschoren</surname><given-names>J.</given-names></name>
</person-group><series>The Springer Series on Challenges in Machine Learning</series><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2019</year><fpage>151</fpage><lpage>160</lpage><isbn>978-3-030-05318-5</isbn></element-citation></ref><ref id="B34-diagnostics-13-02315"><label>34.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Feurer</surname><given-names>M.</given-names></name>
<name><surname>Klein</surname><given-names>A.</given-names></name>
<name><surname>Eggensperger</surname><given-names>K.</given-names></name>
<name><surname>Springenberg</surname><given-names>J.T.</given-names></name>
<name><surname>Blum</surname><given-names>M.</given-names></name>
<name><surname>Hutter</surname><given-names>F.</given-names></name>
</person-group><article-title>Auto-Sklearn: Efficient and Robust Automated Machine Learning</article-title><source>Automated Machine Learning: Methods, Systems, Challenges</source><person-group person-group-type="editor">
<name><surname>Hutter</surname><given-names>F.</given-names></name>
<name><surname>Kotthoff</surname><given-names>L.</given-names></name>
<name><surname>Vanschoren</surname><given-names>J.</given-names></name>
</person-group><series>The Springer Series on Challenges in Machine Learning</series><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2019</year><fpage>113</fpage><lpage>134</lpage><isbn>978-3-030-05318-5</isbn></element-citation></ref><ref id="B35-diagnostics-13-02315"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Dafflon</surname><given-names>J.</given-names></name>
<name><surname>Pinaya</surname><given-names>W.H.L.</given-names></name>
<name><surname>Turkheimer</surname><given-names>F.</given-names></name>
<name><surname>Cole</surname><given-names>J.H.</given-names></name>
<name><surname>Leech</surname><given-names>R.</given-names></name>
<name><surname>Harris</surname><given-names>M.A.</given-names></name>
<name><surname>Cox</surname><given-names>S.R.</given-names></name>
<name><surname>Whalley</surname><given-names>H.C.</given-names></name>
<name><surname>McIntosh</surname><given-names>A.M.</given-names></name>
<name><surname>Hellyer</surname><given-names>P.J.</given-names></name>
</person-group><article-title>An Automated Machine Learning Approach to Predict Brain Age from Cortical Anatomical Measures</article-title><source>Hum. Brain Mapp.</source><year>2020</year><volume>41</volume><fpage>3555</fpage><lpage>3566</lpage><pub-id pub-id-type="doi">10.1002/hbm.25028</pub-id><?supplied-pmid 32415917?><pub-id pub-id-type="pmid">32415917</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="diagnostics-13-02315-f001"><label>Figure 1</label><caption><p>(<bold>Left</bold>): Oligodendroglioma of the right hemisphere, IDH-mutant, ATRX retained, 1p/19q-codeleted, WHO grade 2021: 2 or 3; (<bold>Right</bold>): Semi-automatic segmentation with 3D Slicer.</p></caption><graphic xlink:href="diagnostics-13-02315-g001" position="float"/></fig><fig position="float" id="diagnostics-13-02315-f002"><label>Figure 2</label><caption><p>Flowchart describing the methodological approach. Each model (AutoML with different feature preselection methods and Lasso regression models) is developed 100 times, each time with a new data partitioning. For each of these 100 individual cycles, the preselection of the features, the model development (training), and the determination of the hyperparameters included in the models were performed using the associated individual training data. The hyperparameters were determined using a grid search 10-fold cross-validation in the case of conventional machine learning models and completely automatically in the case of using AutoML (again for each of the 100 individual cycles). The performance of each model is first determined for each individual cycle using the associated independent test data (resulting in a total of 100 individual performance values). Subsequently, the final model performance is calculated as the average of the 100 cycles. Simply put, we fully developed each model with 100 different data partitions 100 times and then calculated the average performance values from those 100 cycles. Steps (<bold>A</bold>&#x02013;<bold>E</bold>,<bold>K</bold>) are performed only once. Steps (<bold>F</bold>&#x02013;<bold>J</bold>), on the other hand, are performed 100 times.</p></caption><graphic xlink:href="diagnostics-13-02315-g002" position="float"/></fig><fig position="float" id="diagnostics-13-02315-f003"><label>Figure 3</label><caption><p>Achieved discriminatory power in predicting IDH mutation status using different feature preselection methods in combination with AutoML as well as a conventional machine learning algorithm. All values were calculated with independent test data and as mean values of 100 cycles. (<bold>Left</bold>): Area under the curve of the receiver operating characteristic (AUC). (<bold>Right</bold>): Area under the precision-recall curve (AUPRC).</p></caption><graphic xlink:href="diagnostics-13-02315-g003" position="float"/></fig><fig position="float" id="diagnostics-13-02315-f004"><label>Figure 4</label><caption><p>Achieved discriminatory power in predicting ATRX status using different feature preselection methods in combination with AutoML as well as a conventional machine learning algorithm. All values were calculated with independent test data and as mean values of 100 cycles. (<bold>Left</bold>): Area under the curve of the receiver operating characteristic (AUC). (<bold>Right</bold>): Area under the precision-recall curve (AUPRC).</p></caption><graphic xlink:href="diagnostics-13-02315-g004" position="float"/></fig><table-wrap position="float" id="diagnostics-13-02315-t001"><object-id pub-id-type="pii">diagnostics-13-02315-t001_Table 1</object-id><label>Table 1</label><caption><p>Demographic characteristics of patients used to test AutoML in terms of predictability of IDH mutation status. The MR images were acquired using the T2 sequence.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Training Data</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Independent Test Data</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Total Data</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Number of patients</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">100</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">24</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">124</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Gender (in %)</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#x02003;Female</td><td align="center" valign="middle" rowspan="1" colspan="1">44.55</td><td align="center" valign="middle" rowspan="1" colspan="1">43.54</td><td align="center" valign="middle" rowspan="1" colspan="1">44.35</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02003;Male</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">55.45</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">56.46</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">55.65</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mean age (in years)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">43.67</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">42.87</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">43.52</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">IDH mutation status (in %)</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#x02003;Wild type (not mutated)</td><td align="center" valign="middle" rowspan="1" colspan="1">35.00</td><td align="center" valign="middle" rowspan="1" colspan="1">33.33</td><td align="center" valign="middle" rowspan="1" colspan="1">34.68</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02003;Mutated</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">65.00</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">66.67</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">65.32</td></tr></tbody></table></table-wrap><table-wrap position="float" id="diagnostics-13-02315-t002"><object-id pub-id-type="pii">diagnostics-13-02315-t002_Table 2</object-id><label>Table 2</label><caption><p>Demographic characteristics of patients used to test AutoML in terms of predictability of ATRX status. The MR images were acquired using the T2 sequence.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Training Data</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Independent Test Data</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Total Data</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Number of patients</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">100</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">24</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">124</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Gender (in %)</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#x02003;Female</td><td align="center" valign="middle" rowspan="1" colspan="1">44.03</td><td align="center" valign="middle" rowspan="1" colspan="1">45.71</td><td align="center" valign="middle" rowspan="1" colspan="1">44.35</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02003;Male</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">55.97</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">54.29</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">55.65</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mean age (in years)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">43.54</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">43.44</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">43.52</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ATRX status (in %)</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#x02003;Retained (not mutated)</td><td align="center" valign="middle" rowspan="1" colspan="1">60.00</td><td align="center" valign="middle" rowspan="1" colspan="1">62.50</td><td align="center" valign="middle" rowspan="1" colspan="1">60.48</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02003;Lost (mutated)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">40.00</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">37.50</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">39.52</td></tr></tbody></table></table-wrap><table-wrap position="float" id="diagnostics-13-02315-t003"><object-id pub-id-type="pii">diagnostics-13-02315-t003_Table 3</object-id><label>Table 3</label><caption><p>Summary of the most important methodological points and algorithms used.</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">Data partitioning:</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02003;&#x02003;80% training data/20% independent test data</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Feature preparation:</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#x02003;&#x02003;z-score transformation</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02003;&#x02003;95% correlation filter</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ML algorithms used (trained with training data):</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#x02003;&#x02003;Lasso regression (best conventional algorithm)</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#x02003;&#x02003;Naive Bayes</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#x02003;&#x02003;LDA</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#x02003;&#x02003;Random forest</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#x02003;&#x02003;Bagged trees</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#x02003;&#x02003;SVM (linear)</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#x02003;&#x02003;SVM (radial)</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#x02003;&#x02003;SVM (polynomial)</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#x02003;&#x02003;Neural net</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">&#x02003;&#x02003;XGBoost</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02003;&#x02003;AutoML (see text for included algorithms)</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Tuning of hyperparameters (based on training data):</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02003;&#x02003;10-fold cross-validation</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Performance metrics used:</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02003;&#x02003;AUC, AUPRC</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Sort metric used (only relevant in case of AutoML):</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02003;&#x02003;AUPRC</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Determination of model performance:</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02003;&#x02003;Based on independent test data</td></tr></tbody></table></table-wrap><table-wrap position="float" id="diagnostics-13-02315-t004"><object-id pub-id-type="pii">diagnostics-13-02315-t004_Table 4</object-id><label>Table 4</label><caption><p>Achieved discriminatory power in predicting IDH mutation status using different feature preselection methods in combination with AutoML as well as a conventional machine learning algorithm. All values were calculated with independent test data and as mean values of 100 cycles. The numbers in brackets indicate the 95% confidence interval. All hyperparameters included in the models were optimized using 10-fold cross-validation (see description in the text). AUC: Area under the curve of the receiver operating characteristic. AUPRC: area under the precision-recall curve. <sup>1</sup> On average, the RFE algorithm selected 19.5 features during the 100 cycles.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Algorithm</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">AUC</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">AUPRC</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Feature preselection with Lasso regression (10 features) + AutoML</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7400 [0.5510, 0.9059]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8524 [0.7211, 0.9527]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Recursive Feature Elimination (19.5 features <sup>1</sup>) + AutoML</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7128 [0.4983, 0.8984]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8319 [0.6571, 0.9540]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No feature preselection (63 features) <break/>+ AutoML</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7400 [0.5041, 0.9326]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8582 [0.6737, 0.9695]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Conventional ML model (15 features)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7242 [0.4889, 0.9248]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8317 [0.6132, 0.9646]</td></tr></tbody></table></table-wrap><table-wrap position="float" id="diagnostics-13-02315-t005"><object-id pub-id-type="pii">diagnostics-13-02315-t005_Table 5</object-id><label>Table 5</label><caption><p>Achieved discriminatory power in predicting ATRX status using different feature preselection methods in combination with AutoML as well as a conventional machine learning algorithm. All values were calculated with independent test data and as mean values of 100 cycles. The numbers in brackets indicate the 95% confidence interval. All hyperparameters included in the models were optimized using 10-fold cross-validation (see description in the text). AUC: Area under the curve of the receiver operating characteristic. AUPRC: Area under the precision-recall curve. <sup>1</sup> On average, the RFE algorithm selected 36.2 features during the 100 cycles.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">
</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin" rowspan="1">Performance Metric</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Algorithm</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AUC</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AUPRC</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Feature preselection with Lasso regression (13 features) + AutoML</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7810 [0.5563, 0.9111]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8511 [0.6603, 0.9522]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Recursive Feature Elimination (36.2 features <sup>1</sup>) + AutoML</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7715 [0.5965, 0.9252]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8502 [0.6867, 0.9576]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No feature preselection (63 features)<break/>+ AutoML</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7628 [0.5630, 0.8998]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8422 [0.6778, 0.9428]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Conventional ML model (13 features)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7744 [0.5446, 0.9185]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8495 [0.6794, 0.9535]</td></tr></tbody></table></table-wrap></floats-group></article>